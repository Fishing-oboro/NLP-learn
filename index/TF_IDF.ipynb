{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDFについて.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNFbhsp20sBlP+yCZhgCwOE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/index/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjCFIqtffo88"
      },
      "source": [
        "# TF-IDF\r\n",
        "***\r\n",
        "Bag-Of-Words(単語の出現数を羅列したベクトル)のように文書内の単語を数えるだけでは、「私」や「これ」といった汎用的な言葉の影響力が大きくなってしまう。  \r\n",
        "そのため、TF-IDFでは多くの文章に出現する単語の値が小さくなるように工夫がされている。\r\n",
        "  \r\n",
        "TF-IDFは**TF**(term frequency)と**IDF**(inverse document frequency)の積を意味する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7yE5xYWq4c"
      },
      "source": [
        "## TF (Term Frequency)\r\n",
        "---\r\n",
        "ある文章においてその単語が出現した割合を示す。一つの文章中に多く出現している単語ほど文章に関わるという考え方に基づいており、この値が大きいほどその単語が重要であるといえる。\r\n",
        "\r\n",
        "　　\r\n",
        "$$\r\n",
        "tf(t,d) = \\frac{n_{t,d}}{\\sum_{s\\in{d}}n_{s,d}}\\\\\r\n",
        "(tf(t,d) : 文書d中の単語tのTF値\\quad\r\n",
        "n_{t,d} : 文書d中の単語tの出現数\\quad\r\n",
        "\\sum_{s\\in{d}}n_{s,d}: 文書d中の全単語の出現数の和)\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_NcmuYMeYdf"
      },
      "source": [
        "## IDF (Inverse Document Frequency)\r\n",
        "---\r\n",
        "ある文章においてその単語が出現した割合を示す。この値が大きいほどその単語が重要であるといえる。  \r\n",
        "\\\r\n",
        "$$idf(t) = \\log{\\frac{N}{df(t)}}+1\\\\\r\n",
        "(idf(t) : 単語tのIDF値\\quad\r\n",
        "N : 全文書数\\quad\r\n",
        "df(t): 単語tを含む文書数)\r\n",
        "$$\r\n",
        "\\\r\n",
        "後述のTfidfVectorizerではlog内の数が0にならないようにN及びdf(t)に1を加えた値が用いられている。  \r\n",
        "\\\r\n",
        "$$idf(t) = \\log{\\frac{N+1}{df(t)+1}}+1\\\\\r\n",
        "(idf(t) : 単語tのIDF値\\quad\r\n",
        "N : 全文書数\\quad\r\n",
        "df(t): 単語tを含む文書数)\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQQDM9Z2-p-6"
      },
      "source": [
        "## cos類似度\r\n",
        "---\r\n",
        "単語や文章をベクトルで表すことで、以下の方程式に当てはめることができる。  \r\n",
        "\r\n",
        "\r\n",
        "$$\r\n",
        "\\cos(\\vec{a}, \\vec{b})=\\frac{\\vec{a}・\\vec{b}}{|\\vec{a}||\\vec{b}|}\r\n",
        "=\\frac{\\sum{a_ib_i}}{\\sqrt{\\sum{a_i^2}}・\\sqrt{\\sum{b_i^2}}}\r\n",
        "$$\r\n",
        "\r\n",
        "cosの性質からcos(a,b)が1に近いほどa,bの性質が類似しており、0に近ければ類似していないといえる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMYPYcbiIAe7"
      },
      "source": [
        "## 実装例\r\n",
        "---\r\n",
        "実際にTF-IDFを用いて、分散表現を作成する。\r\n",
        "文章は[ferret](https://ferret-plus.com/)の記事5件とその関連記事1件を使用する。\r\n",
        "まず必要なファイルをダウンロードした後にmecabを使い記事の形態素解析を行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPC6FEJqKy78"
      },
      "source": [
        "# mecabに必要なライブラリ取得\r\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\r\n",
        "!pip install mecab-python3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL9w-0NC5UfW"
      },
      "source": [
        "# テキストの取得\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/documents.txt\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/titles.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xiuP7Hpq3k"
      },
      "source": [
        "実装にはsklearnに搭載されているTfidfVectorizerとcosine_similarityを用いる。\r\n",
        "これらを使うことで上に挙げた演算を容易に実装することができる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCaxsE977-Yi",
        "outputId": "cc999224-6ef3-4dac-9fa5-d7ced5f5e77e"
      },
      "source": [
        "import numpy as np\r\n",
        "import MeCab\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "# input_text = open('./titles.txt', 'r').read()\r\n",
        "input_text = open('./documents.txt', 'r').read()\r\n",
        "documents = input_text.split(\"|\")\r\n",
        "\r\n",
        "def words(text):\r\n",
        "    \"\"\"\r\n",
        "        文章から単語を抽出\r\n",
        "    \"\"\"\r\n",
        "    out_words = []\r\n",
        "    tagger = MeCab.Tagger('-Ochasen')\r\n",
        "    tagger.parse('')\r\n",
        "    node = tagger.parseToNode(text)\r\n",
        "\r\n",
        "    while node:\r\n",
        "        word_type = node.feature.split(\",\")[0]\r\n",
        "        if word_type in [\"名詞\"]:\r\n",
        "            out_words.append(node.surface)\r\n",
        "        node = node.next\r\n",
        "    return out_words\r\n",
        "\r\n",
        "\r\n",
        "def vecs_array(documents):\r\n",
        "    \"\"\"\r\n",
        "    各文章における重み付け\r\n",
        "    \"\"\"\r\n",
        "    docs = np.array(documents)\r\n",
        "    vectorizer = TfidfVectorizer(\r\n",
        "        analyzer=words,\r\n",
        "        stop_words='|',\r\n",
        "        min_df=1,\r\n",
        "        token_pattern='(?u)\\\\b\\\\w+\\\\b' #文字列長が1の単語を処理対象に含める\r\n",
        "    )\r\n",
        "    vecs = vectorizer.fit_transform(docs)\r\n",
        "    print(vecs.shape) # (ベクトルの数, 次元数)\r\n",
        "    return vecs.toarray()\r\n",
        "\r\n",
        "# Cos類似度\r\n",
        "input_title = open('./titles.txt', 'r').read()\r\n",
        "tag = [\"記事A\", \"記事B\", \"記事C\", \"記事D\", \"記事E\", \"記事F\"]\r\n",
        "titles = input_title.split(\"|\")\r\n",
        "\r\n",
        "vecs = vecs_array(documents)\r\n",
        "print(vecs) # ベクトルの構造\r\n",
        "cs_array = cosine_similarity(vecs, vecs)\r\n",
        "\r\n",
        "for i, cs_item in enumerate(cs_array):\r\n",
        "    print(tag[i] + \":\" + titles[i])\r\n",
        "    cs_dic = {}\r\n",
        "    for j, cs in enumerate(cs_item):\r\n",
        "        if round(cs - 1.0, 5) != 0: #同じ文書同士は省く\r\n",
        "            cs_dic[tag[j]] = cs\r\n",
        "    for k, v in sorted(cs_dic.items(), key=lambda x:x[1], reverse=True):\r\n",
        "        print(\"\\t\" + k + \" : \" + str(v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6, 978)\n",
            "[[0.24147932 0.         0.         ... 0.         0.         0.01724852]\n",
            " [0.         0.         0.         ... 0.         0.01314067 0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.18264125 ... 0.04949541 0.         0.        ]\n",
            " [0.         0.0464845  0.03811796 ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
            "記事A:知りたい情報にすぐたどり着ける！Yahoo!・Googleの便利な検索術を解説\n",
            "\t記事F : 0.10376290250287298\n",
            "\t記事E : 0.09985177786264934\n",
            "\t記事B : 0.05427688291534725\n",
            "\t記事D : 0.05294733505828748\n",
            "\t記事C : 0.000780527531561328\n",
            "記事B:\n",
            "実際何が違うの？IoTとM2Mの定義の解説＆事例紹介\n",
            "\t記事F : 0.09795495964706279\n",
            "\t記事D : 0.07281934665561288\n",
            "\t記事E : 0.07261294739068644\n",
            "\t記事A : 0.05427688291534725\n",
            "\t記事C : 0.010970631786312737\n",
            "記事C:\n",
            "「“α Plaza”オープン」 プロフェッショナルをはじめとした全てのαユーザーの創作活動をワンストップで支援\n",
            "\t記事F : 0.04012718265656499\n",
            "\t記事D : 0.026316557315514903\n",
            "\t記事E : 0.012546047754696943\n",
            "\t記事B : 0.010970631786312737\n",
            "\t記事A : 0.000780527531561328\n",
            "記事D:\n",
            "ピツニーボウズとLOCUSが対話式パーソナライズド動画ソリューションで協業を開始\n",
            "\t記事F : 0.2295366757686643\n",
            "\t記事E : 0.13527574532791908\n",
            "\t記事B : 0.07281934665561288\n",
            "\t記事A : 0.05294733505828748\n",
            "\t記事C : 0.026316557315514903\n",
            "記事E:\n",
            "Facebookが10月にリリースした新サービス！法人版の社内SNS「Workplace」を使う3つのメリット\n",
            "\t記事F : 0.4259090900333243\n",
            "\t記事D : 0.13527574532791908\n",
            "\t記事A : 0.09985177786264934\n",
            "\t記事B : 0.07261294739068644\n",
            "\t記事C : 0.012546047754696943\n",
            "記事F:\n",
            "今年Facebookから発表された新機能5つまとめ\n",
            "\n",
            "\t記事E : 0.4259090900333243\n",
            "\t記事D : 0.2295366757686643\n",
            "\t記事A : 0.10376290250287298\n",
            "\t記事B : 0.09795495964706279\n",
            "\t記事C : 0.04012718265656499\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}