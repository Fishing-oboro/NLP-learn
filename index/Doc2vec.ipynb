{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doc2vec.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPk6w4Yd5rKOZOZUFvs0Ef5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/index/Doc2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-Y3Xtc2c97D"
      },
      "source": [
        "# Doc2vec (Paragraph Vector)\r\n",
        "---\r\n",
        "カウントベースの手法によって、文章ベクトルの獲得が可能である。しかし、その手法は単語の語順や意味を表現することができないという欠点を持つ。Doc2vecでは、その欠点を克服するためにword2vecの手法を文章ベクトルの獲得に取り入れている。  \r\n",
        "これには、PV-DBOWとPV-DMという二つの手法があり、word2vecで用いられる2つの手法に類似している。\r\n",
        "## Doc2vecの特徴\r\n",
        "---\r\n",
        "1. 可変長の文章から固定長の文章ベクトルを得ることができる。\r\n",
        "2. word2vecと同様に単語を予測する過程でベクトルを得る。\r\n",
        "3. word2vecとほぼ同一のニューラルネットワークを用いて学習する。\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thHWvXyDi9kA"
      },
      "source": [
        "## PV-DM(分散記憶モデル)\r\n",
        "---\r\n",
        "この手法は、**CBOWと同様に周辺単語(コンテキスト)から中心単語(ターゲット)**を推測することを目的としたニューラルネットワークを活用する。入力層を**周辺単語ベクトル＋文章ベクトル**とすることで文章全体を加味したベクトルを得ることができる。\r\n",
        "\r\n",
        "#### NNの構造\r\n",
        "---\r\n",
        "![](https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/image/PV%E3%83%BCDM.jpg)\r\n",
        "1. 入力層:コンテキストに重みWin、文章IDに重みDinを使って低次元のベクトルに変換\r\n",
        "2. 中間層:複数のベクトルを結合(和、平均など)\r\n",
        "3. 出力層:重みを使って元の次元のベクトルに変換し、softMax関数で確率化。\r\n",
        "4. 誤差逆伝搬:正解ラベルと確率の誤差が小さくなるように重みDinを更新。\r\n",
        "\r\n",
        "\r\n",
        "#### PV-DMの特徴\r\n",
        "---\r\n",
        "周辺単語をベクトル化する際にWinを必要とするため、あらかじめ文章中の全単語の単語ベクトルを手に入れておく必要がある。また、文章ベクトルはDinから得ることができ、word2vecにおけるWinと同様のものである。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVf5lPHhLpen"
      },
      "source": [
        "## PV-DBOW(分散BOWモデル)\r\n",
        "--- \r\n",
        "\r\n",
        "この手法は、**文書IDから文中の単語(ターゲット)**を推測することを目的としたニューラルネットワークを活用する。入力層が文章IDのみであるため、**単語ベクトルを必要としない**。  \r\n",
        "しかし、語順を無視して文中の単語を予測するため、文脈を加味しないベクトルになる。\r\n",
        "#### NNの構造 \r\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8scIlEYeqkPs"
      },
      "source": [
        "![](https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/image/PV-DBOW.jpg)\r\n",
        "\r\n",
        "1. 入力層:文章IDに重みDinを使って低次元のベクトルに変換\r\n",
        "2. 中間層:ベクトルを分割\r\n",
        "3. 出力層:重みを使って元の次元のベクトルに変換し、softMax関数で確率化。\r\n",
        "4. 誤差逆伝搬:正解ラベルと確率の誤差が小さくなるように重みDinを更新。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh9WNbXwNnsH"
      },
      "source": [
        "## 実装例\r\n",
        "---\r\n",
        "実際にDoc2vecを用いて、分散表現を作成する。 文章はTF-IDfの例と同様に[ferret](https://ferret-plus.com/)の記事5件とその関連記事1件を使用する。 まず必要なファイルをダウンロードした後にmecabを使い記事の形態素解析を行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIkUIVWfXf8U"
      },
      "source": [
        "# mecabに必要なライブラリ取得\r\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\r\n",
        "!pip install mecab-python3\r\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc # シンボリックリンクの作成"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVKTeJ01Xg1Z"
      },
      "source": [
        "# テキストの取得\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/documents.txt\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/titles.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC1NjZAHarLr"
      },
      "source": [
        "実装にはgensimに搭載されているDoc2VecrとTaggedDocumentを用いる。 これらを使うことで上に挙げた演算を容易に実装することができる。\r\n",
        "DOC2Vecのパラメータについては[doc2vecパラメータ](https://www.gmo-jisedai.com/wp-content/uploads/%E3%82%B9%E3%82%AF%E3%83%AA%E3%83%BC%E3%83%B3%E3%82%B7%E3%83%A7%E3%83%83%E3%83%88-2019-02-13-14.41.48.png)に記載"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddSQ5Z9qXkf6"
      },
      "source": [
        "import numpy as np\r\n",
        "import MeCab\r\n",
        "import sys\r\n",
        "from gensim.models.doc2vec import Doc2Vec\r\n",
        "from gensim.models.doc2vec import TaggedDocument\r\n",
        "from collections import OrderedDict\r\n",
        "\r\n",
        "# input_text = open('./titles.txt', 'r').read()\r\n",
        "input_text = open('./documents.txt', 'r').read()\r\n",
        "documents = input_text.split(\"|\")\r\n",
        "\r\n",
        "def words(text):\r\n",
        "    \"\"\"\r\n",
        "        文章から単語を抽出\r\n",
        "    \"\"\"\r\n",
        "    out_words = []\r\n",
        "    tagger = MeCab.Tagger('-Ochasen')\r\n",
        "    tagger.parse('')\r\n",
        "    node = tagger.parseToNode(text)\r\n",
        "\r\n",
        "    while node:\r\n",
        "        word_type = node.feature.split(\",\")[0]\r\n",
        "        if word_type in [\"名詞\"]:\r\n",
        "            out_words.append(node.surface)\r\n",
        "        node = node.next\r\n",
        "    return out_words\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "# 学習データとなる各文書\r\n",
        "training_docs = []\r\n",
        "for i, document in enumerate(documents):\r\n",
        "    training_docs.append(TaggedDocument(words=words(document), tags=['doc' + str(i + 1)]))\r\n",
        "\r\n",
        "# min_count=1:最低1回出現した単語を学習に使用\r\n",
        "# dm: 学習モデル=0,DBOW 1,DM\r\n",
        "model = Doc2Vec(training_docs)\r\n",
        "\r\n",
        "tags = OrderedDict() #辞書の繰り返し時による順番を保つ\r\n",
        "tag_list = (('doc1', \"記事A\"), ('doc2', \"記事B\"), ('doc3', \"記事C\"), ('doc4', \"記事D\"), ('doc5', \"記事E\"), ('doc6', \"記事F\"))\r\n",
        "dic = OrderedDict(tag_list)\r\n",
        "tags.update(dic)\r\n",
        "\r\n",
        "for k, v in tags.items():\r\n",
        "    print(\"[\" + v + \"]\")\r\n",
        "    for items in model.docvecs.most_similar(k):\r\n",
        "        print(\"\\t\" + tags[items[0]] + \" : \"+ str(items[1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}