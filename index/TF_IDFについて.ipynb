{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDFについて.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM7tDoeWoJQ/ZVXavvr5CaA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/TF_IDF%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPDLZwmyop5W"
      },
      "source": [
        "# カウントベース\r\n",
        "---\r\n",
        "## BoW(Bag-of-Words)\r\n",
        "---\r\n",
        "bag of words(以下BoW)はカウントベースの手法に属しており、文書中に出現する単語の数を数え上げることからその名前が付けられている。\r\n",
        "BoWでは文書中に出現する全ての単語にidを割り当て、各単語の出現数を記録する。そして出現数を対応する単語のidに割り当てることで文書ベクトルとする。\r\n",
        "$$私[0]　は[1]　リンゴ[2]　と[3]　リンゴ[2]　を[4]　食べる[5]　。[6]→文書[1,1,2,1,1,1,1]$$\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjCFIqtffo88"
      },
      "source": [
        "## TF-IDF\r\n",
        "***\r\n",
        "Bag-Of-Wordsのように文書内の単語を数えるだけでは、「私」や「これ」といった汎用的な言葉の影響力が大きくなってしまう。  \r\n",
        "そのため、TF-IDFでは多くの文章に出現する単語の値が小さくなるように工夫がされている。\r\n",
        "  \r\n",
        "TF-IDFは**TF**(term frequency)と**IDF**(inverse document frequency)の積を意味する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf7yE5xYWq4c"
      },
      "source": [
        "### TF (Term Frequency)\r\n",
        "---\r\n",
        "ある文章においてその単語が出現した割合を示す。一つの文章中に多く出現している単語ほど文章に関わるという考え方に基づいており、この値が大きいほどその単語が重要であるといえる。\r\n",
        "\r\n",
        "　　\r\n",
        "$$\r\n",
        "tf(t,d) = \\frac{n_{t,d}}{\\sum_{s\\in{d}}n_{s,d}}\\\\\r\n",
        "(tf(t,d) : 文書d中の単語tのTF値\\quad\r\n",
        "n_{t,d} : 文書d中の単語tの出現数\\quad\r\n",
        "\\sum_{s\\in{d}}n_{s,d}: 文書d中の全単語の出現数の和)\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_NcmuYMeYdf"
      },
      "source": [
        "### IDF (Inverse Document Frequency)\r\n",
        "---\r\n",
        "ある文章においてその単語が出現した割合を示す。この値が大きいほどその単語が重要であるといえる。  \r\n",
        "\\\r\n",
        "$$idf(t) = \\log{\\frac{N}{df(t)}}+1\\\\\r\n",
        "(idf(t) : 単語tのIDF値\\quad\r\n",
        "N : 全文書数\\quad\r\n",
        "df(t): 単語tを含む文書数)\r\n",
        "$$\r\n",
        "\\\r\n",
        "後述のTfidfVectorizerではlog内の数が0にならないようにN及びdf(t)に1を加えた値が用いられている。  \r\n",
        "\\\r\n",
        "$$idf(t) = \\log{\\frac{N+1}{df(t)+1}}+1\\\\\r\n",
        "(idf(t) : 単語tのIDF値\\quad\r\n",
        "N : 全文書数\\quad\r\n",
        "df(t): 単語tを含む文書数)\r\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQQDM9Z2-p-6"
      },
      "source": [
        "### cos類似度\r\n",
        "---\r\n",
        "単語や文章をベクトルで表すことで、以下の方程式に当てはめることができる。  \r\n",
        "\r\n",
        "\r\n",
        "$$\r\n",
        "\\cos(\\vec{a}, \\vec{b})=\\frac{\\vec{a}・\\vec{b}}{|\\vec{a}||\\vec{b}|}\r\n",
        "=\\frac{\\sum{a_ib_i}}{\\sqrt{\\sum{a_i^2}}・\\sqrt{\\sum{b_i^2}}}\r\n",
        "$$\r\n",
        "\r\n",
        "cosの性質からcos(a,b)が1に近いほどa,bの性質が類似しており、0に近ければ類似していないといえる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMYPYcbiIAe7"
      },
      "source": [
        "### 実装例\r\n",
        "---\r\n",
        "実際にTF-IDFを用いて、分散表現を作成する。\r\n",
        "文章は[ferret](https://ferret-plus.com/)の記事5件とその関連記事1件を使用する。\r\n",
        "まず必要なファイルをダウンロードした後にmecabを使い記事の形態素解析を行う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPC6FEJqKy78"
      },
      "source": [
        "# mecabに必要なライブラリ取得\r\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\r\n",
        "!pip install mecab-python3\r\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc # シンボリックリンクの作成"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL9w-0NC5UfW"
      },
      "source": [
        "# テキストの取得\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/documents.txt\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/titles.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4xiuP7Hpq3k"
      },
      "source": [
        "実装にはsklearnに搭載されているTfidfVectorizerとcosine_similarityを用いる。\r\n",
        "これらを使うことで上に挙げた演算を容易に実装することができる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCaxsE977-Yi"
      },
      "source": [
        "import numpy as np\r\n",
        "import MeCab\r\n",
        "import sys\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "# input_text = open('./titles.txt', 'r').read()\r\n",
        "input_text = open('./documents.txt', 'r').read()\r\n",
        "documents = input_text.split(\"|\")\r\n",
        "\r\n",
        "def words(text):\r\n",
        "    \"\"\"\r\n",
        "        文章から単語を抽出\r\n",
        "    \"\"\"\r\n",
        "    out_words = []\r\n",
        "    tagger = MeCab.Tagger('-Ochasen')\r\n",
        "    tagger.parse('')\r\n",
        "    node = tagger.parseToNode(text)\r\n",
        "\r\n",
        "    while node:\r\n",
        "        word_type = node.feature.split(\",\")[0]\r\n",
        "        if word_type in [\"名詞\"]:\r\n",
        "            out_words.append(node.surface)\r\n",
        "        node = node.next\r\n",
        "    return out_words\r\n",
        "\r\n",
        "\r\n",
        "def vecs_array(documents):\r\n",
        "    \"\"\"\r\n",
        "    各文章における重み付け\r\n",
        "    \"\"\"\r\n",
        "    docs = np.array(documents)\r\n",
        "    vectorizer = TfidfVectorizer(\r\n",
        "        analyzer=words,\r\n",
        "        stop_words='|',\r\n",
        "        min_df=1,\r\n",
        "        token_pattern='(?u)\\\\b\\\\w+\\\\b' #文字列長が1の単語を処理対象に含める\r\n",
        "    )\r\n",
        "    vecs = vectorizer.fit_transform(docs)\r\n",
        "    print(vecs.shape) # (ベクトルの数, 次元数)\r\n",
        "    return vecs.toarray()\r\n",
        "\r\n",
        "# Cos類似度\r\n",
        "input_title = open('./titles.txt', 'r').read()\r\n",
        "tag = [\"記事A\", \"記事B\", \"記事C\", \"記事D\", \"記事E\", \"記事F\"]\r\n",
        "titles = input_title.split(\"|\")\r\n",
        "\r\n",
        "vecs = vecs_array(documents)\r\n",
        "print(vecs) # ベクトルの構造\r\n",
        "cs_array = cosine_similarity(vecs, vecs)\r\n",
        "\r\n",
        "for i, cs_item in enumerate(cs_array):\r\n",
        "    print(tag[i] + \":\" + titles[i])\r\n",
        "    cs_dic = {}\r\n",
        "    for j, cs in enumerate(cs_item):\r\n",
        "        if round(cs - 1.0, 5) != 0: #同じ文書同士は省く\r\n",
        "            cs_dic[tag[j]] = cs\r\n",
        "    for k, v in sorted(cs_dic.items(), key=lambda x:x[1], reverse=True):\r\n",
        "        print(\"\\t\" + k + \" : \" + str(v))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
