{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDOYHwUvZs0eSaH1aAzK+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKB114IS7Vv4"
      },
      "source": [
        "# word2vec (推論ベース)\r\n",
        "***\r\n",
        "word2vecによって単語の分散表現(ベクトル)を得ることができる。単語をベクトルして扱うことでcos類似度や加算減算への利用が可能になる。\r\n",
        "これには、**CBOW**と**skip-gram**という二つの手法がある。二つの手法はどちらも**分布仮説**に基づいており、一つの単語とその周辺の単語の関係性から特徴量を得る。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeNy2SDU3EtJ"
      },
      "source": [
        "## CBOW(Continious Bag of Words)\r\n",
        "***\r\n",
        "この手法では**周辺単語(コンテキスト)から中心単語(ターゲット)**を推測することを目的としたニューラルネットワークを活用する。ニューラルネットワークの推論精度が高まるほど正確な単語分散表現を得ることができる。このCBOWの仕組みについて以下の文章を例に考える。  \r\n",
        "\r\n",
        "　　　　　　　**「私　は　リンゴ　を　食べる　。」**\r\n",
        "\r\n",
        "まず、使用される全単語をもった辞書を作成し、**one-hotベクトル**として扱う。\r\n",
        "\r\n",
        "$$\r\n",
        "例：(私→id:0　\\begin{matrix}(1&0&0&0&0&0)\\end{matrix}, 　リンゴ→id:2　\\begin{matrix}(0&0&1&0&0&0)\\end{matrix})\r\n",
        "$$\r\n",
        "\r\n",
        "次に一つの単語とその周辺単語の組を作成し、それぞれを下記のような正解ラベル、コンテキストとして扱う。\r\n",
        "\r\n",
        "| コンテキスト(入力) | 正解ラベル(出力) | \r\n",
        "| ---- | :----: |\r\n",
        "| 私[0], 　　　リンゴ[2] | は[1] |\r\n",
        "| は[1], 　　　を[3] | リンゴ[2] |\r\n",
        "| リンゴ[2],　食べる[4] | を[3] |\r\n",
        "| を[3],　　　。[5] | 食べる[4] |\r\n",
        "\r\n",
        "この表のデータを利用して下記のようなニューラルネットワークを作成し、Winを単語分散表現として得ることができる。このときニューラルネットワークの出力はスコアを示しているが、softMax関数を利用することで各行の重みの和を1とすることができ、正答単語の確率として扱うことができる。この確率と正解ラベルの誤差を小さくすることで推測の精度を高めることができる。\r\n",
        "#### NNの構造\r\n",
        "---\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/image/CBOW.jpg\" width=80%>\r\n",
        "</div>\r\n",
        "\r\n",
        "1. 入力層:コンテキストを重みWinを使って低次元のベクトルに変換\r\n",
        "2. 中間層:複数のベクトルを結合(和、平均など)\r\n",
        "3. 出力層:重みを使って元の次元のベクトルに変換し、softMax関数で確率化。\r\n",
        "4. 誤差逆伝搬:正解ラベルと確率の誤差が小さくなるように重みを更新。\r\n",
        "\r\n",
        "#### $W_{in}$がなぜ単語分散表現になるのか\r\n",
        "---\r\n",
        "私$\\begin{matrix}(1&0&0&0&0&0)\\end{matrix}$とWinの積は以下のような式になる。  \r\n",
        "\r\n",
        "$$\r\n",
        "\\left(\\begin{matrix}1&0&0&0&0&0\\end{matrix}\\right)\r\n",
        "×\r\n",
        "\\left(\\begin{matrix}\r\n",
        "w_{11}&w_{12}&...&w_{1n}\\\\\r\n",
        "w_{21}&w_{22}&...&w_{2n}\\\\\r\n",
        "\\\\\r\n",
        "w_{61}&w_{62}&...&w_{6n}\\\\\r\n",
        "\\end{matrix}\\right)\r\n",
        "\r\n",
        "\r\n",
        "=\\begin{matrix}(w_{11}&w_{12}&...&w_{1n})\\end{matrix}\r\n",
        "$$\r\n",
        "  \r\n",
        "\r\n",
        "このことからone-hot表現と重み行列の積は単語を示すidの行を取り出すことを意味していることがわかる。したがって、重み行列の各行は辞書に登録されている単語を示す。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bisP-72MAMSa"
      },
      "source": [
        "\r\n",
        "## skip-gram\r\n",
        "***\r\n",
        "この手法ではCBOWとは逆に**中心単語から周辺単語**を推測するニューラルネットワークによって分散表現を得る。このskip-gramについてCBOWで用いた物と同様の例文を用いて考える。\r\n",
        "\r\n",
        " \r\n",
        "　　　　　　　**「私　は　リンゴ　を　食べる　。」**\r\n",
        "\r\n",
        "CBOWと同様に使用される全単語をもった辞書を作成し、one-hotベクトルとして扱う。\r\n",
        "\r\n",
        "$$\r\n",
        "例：(私→id:0　\\begin{matrix}(1&0&0&0&0&0)\\end{matrix}, 　リンゴ→id:2　\\begin{matrix}(0&0&1&0&0&0)\\end{matrix})\r\n",
        "$$\r\n",
        "\r\n",
        "次に一つの単語とその周辺単語の組を作成し、それぞれを下記のような正解ラベル、コンテキストとして扱う。\r\n",
        "\r\n",
        "| 入力 | 正解ラベル(出力) | \r\n",
        "| :----: | ---- |\r\n",
        "| は[1]  |私[0], 　　　リンゴ[2] |\r\n",
        "| リンゴ[2]  |は[1], 　　　を[3] |\r\n",
        "| を[3]  |リンゴ[2],　食べる[4] |\r\n",
        "| 食べる[4]  |を[3],　　　。[5] |\r\n",
        "\r\n",
        "この表のデータを利用して下記のようなニューラルネットワークを作成し、Winを単語分散表現として得ることができる。このときニューラルネットワークの出力はスコアを示しているが、softMax関数を利用することで各行の重みの和を1とすることができ、正答単語の確率として扱うことができる。この確率と正解ラベルの誤差を小さくすることで推測の精度を高めることができる。\r\n",
        "\r\n",
        "#### NNの構造\r\n",
        "---\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/image/skipgram.p.jpg\" width=80%>\r\n",
        "</div>\r\n",
        "\r\n",
        "1. 入力層:単語を重みWinを使って低次元のベクトルに変換\r\n",
        "2. 中間層:\r\n",
        "3. 出力層:重みを使って元の次元のベクトルに変換し、softMax関数で確率化。\r\n",
        "4. 誤差逆伝搬:正解ラベルと確率の誤差が小さくなるように重みを更新。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gXVdfACiT5z"
      },
      "source": [
        "## 実装例\r\n",
        "---\r\n",
        "例文には青空文庫の[こころ(夏目漱石)](https://www.aozora.gr.jp/cards/000148/card773.html#download)を利用する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKuyHEtSQ0r"
      },
      "source": [
        "mecabの出力フォーマットのオプションについては[MeCabのコマンドライン引数一覧とその実行例](http://www.mwsoft.jp/programming/munou/mecab_command.html)に詳しく記載されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5dyjauxiTjz"
      },
      "source": [
        "# mecabに必要なライブラリ取得\r\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\r\n",
        "!pip install mecab-python3\r\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LaRAAXoQ9c6"
      },
      "source": [
        "# テキストの取得\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/kokoro.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68LESIchvQIF"
      },
      "source": [
        "# テキストの加工\r\n",
        "with open(\"./kokoro.txt\", mode=\"r\") as f:\r\n",
        "    text = f.readlines()\r\n",
        "    for line in text:\r\n",
        "        if \"。\" in line:\r\n",
        "          with open(\"./kokoro_new.txt\", mode=\"a\") as of:\r\n",
        "            of.write(line)\r\n",
        "\r\n",
        "# 基本形で分かち書きにする\r\n",
        "!mecab -F\"%f[6] \" -U\"%m \" -E\"\\n\" -o kokoro_wakati.txt kokoro_new.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0b4qUAeq6_"
      },
      "source": [
        "Word2Vecの各オプションについては[gemsimのword2vecのためのオプション一覧](https://onemuri.space/note/1ceozcpdt/)に記載されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OQdMbYU658h"
      },
      "source": [
        "# モデルの作成\r\n",
        "from gensim.models import word2vec\r\n",
        "docs = word2vec.LineSentence(\"kokoro_wakati.txt\") \r\n",
        "model = word2vec.Word2Vec(docs,\r\n",
        "                        size=100,\r\n",
        "                        min_count=1,\r\n",
        "                        window=5,\r\n",
        "                        iter=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A5D-0PV2DxI"
      },
      "source": [
        "# 類義語の確認\r\n",
        "for i in model.most_similar('母'):\r\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hphI8Jdn5opH"
      },
      "source": [
        "## 学習済みモデルの利用\r\n",
        "---\r\n",
        "word2vecを利用した単語分散表現の取得には、自身で用意した大量の文章データから作成する方法の他に、学習済みモデルを利用する方法がある。\r\n",
        "この方法ではmodelの作成にWord2Vec()ではなく、load()やload_word2vec_format()を用いてモデルを読み込む。\r\n",
        "\r\n",
        "#### 学習済みモデルの例\r\n",
        "---\r\n",
        "- [WikiEntVec 東北大学 乾・鈴木研究室](http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/)  \r\n",
        "wikipediaの記事になっている単語とそのリンクを関連付けることで汎用単語と固有単語を見分ける工夫がされている。  \r\n",
        "コーパス:Wikipedia、形態素解析:mecab、次元:100~300\r\n",
        "- [白ヤギコーポレーション](https://aial.shiroyagi.co.jp/2017/02/japanese-word2vec-model-builder/)  \r\n",
        "モデルが小さいため、比較的軽量。表現力は他モデルにやや劣る。  \r\n",
        "コーパス:Wikipedia、形態素解析:mecab、次元:50\r\n",
        "- [chiVe(チャイブ) 徳島人工知能NLP研究所 国立国語研究所](https://github.com/WorksApplications/chiVe)    \r\n",
        "コーパス:NWJC、形態素解析:Sudachi、次元:300\r\n",
        "\r\n",
        "#### 実装例(白ヤギ)\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWmo2t6dCH--"
      },
      "source": [
        "!wget http://public.shiroyagi.s3.amazonaws.com/latest-ja-word2vec-gensim-model.zip\r\n",
        "!unzip ./latest-ja-word2vec-gensim-model.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjyGOZmSEmue"
      },
      "source": [
        "from gensim.models import word2vec\r\n",
        "\r\n",
        "model = word2vec.Word2Vec.load('./word2vec.gensim.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYgWsXaYI-3y"
      },
      "source": [
        "for i in model.most_similar('母'):\r\n",
        "    print(i)\r\n",
        "print('\\n')\r\n",
        "for i in model.most_similar(positive=['女','美しい'] negative=['']):\r\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAuD5dJoH3DM"
      },
      "source": [
        "### word2vecを使った文章のベクトル化手法の例\r\n",
        "---\r\n",
        "1. 文章に登場する単語のベクトルの平均を利用する。  \r\n",
        "2. TF-IDFと組み合わせる。\r\n",
        "\r\n"
      ]
    }
  ]
}
