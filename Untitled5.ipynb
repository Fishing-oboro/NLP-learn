{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMy8dCN3ydnVZvSrR+Flty+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl5X2s36PuQ5",
        "outputId": "2da34268-6399-454f-8051-688403176c94"
      },
      "source": [
        "!pip install beautifulsoup4"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhqR_6uNR3z2"
      },
      "source": [
        "from bs4 import BeautifulSoup\r\n",
        "import requests\r\n",
        "import time\r\n",
        "\r\n",
        "categories = ['dom', 'world', 'eco', 'ent', 'sports', 'gourmet', 'love', 'trend']\r\n",
        "base_url = \"https://news.livedoor.com/topics/category/\"\r\n",
        "headers = {\r\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36 Edg/89.0.774.48\"\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def soup(url):\r\n",
        "  time.sleep(3)\r\n",
        "  headers = {\r\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36 Edg/89.0.774.48\"\r\n",
        "  }\r\n",
        "  \r\n",
        "\r\n",
        "  html = requests.get(url, headers = headers)\r\n",
        "  soup = BeautifulSoup(html.content, \"html.parser\")\r\n",
        "  return soup\r\n",
        "\r\n",
        "\r\n",
        "# カテゴリごとのURL取得\r\n",
        "urllist = []\r\n",
        "# page数指定\r\n",
        "page = 2\r\n",
        "\r\n",
        "for i, category  in enumerate(categories):\r\n",
        "  print(categories[i])\r\n",
        "  category_url = base_url + categories[i] + '/'\r\n",
        "\r\n",
        "  # 1page分\r\n",
        "  urls = [topic_soup.find('a').attrs[\"href\"].replace('topics', 'article') for topic_soup in soup(category_url).findAll('li', {'class': 'hasImg'})]\r\n",
        "  # 1のときは無し、2から追加\r\n",
        "  n = 2\r\n",
        "  while(page >= n):\r\n",
        "    print('start')\r\n",
        "    next = '?p=' + str(n)\r\n",
        "    next_url = category_url + next\r\n",
        "    a_urls = [topic_soup.find('a').attrs[\"href\"].replace('topics', 'article') for topic_soup in soup(next_url).findAll('li', {'class': 'hasImg'})]\r\n",
        "    urls.extend(a_urls)\r\n",
        "    n += 1\r\n",
        "\r\n",
        "  urllist.append(urls)\r\n",
        "  print(urls)\r\n",
        "\r\n",
        "print(urllist)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Ir2Z5D-USs"
      },
      "source": [
        "\r\n",
        "# 複数ページの読み込み　←ココ\r\n",
        "# カテゴリ、タイトル、本文、取得\r\n",
        "d = {'\\u3000': None, '\\n': None, ' ': None}\r\n",
        "trans = str.maketrans(d)\r\n",
        "\r\n",
        "articles = []\r\n",
        "for number, urls in enumerate(urllist):\r\n",
        "  print(categories[number])\r\n",
        "  for url in urls:\r\n",
        "    try:\r\n",
        "      soup_obj = soup(url)\r\n",
        "      if soup_obj.original_encoding == 'windows-1252':\r\n",
        "        print('error')\r\n",
        "        continue\r\n",
        "\r\n",
        "      title = soup_obj.find('h1', {'class': 'articleTtl'}).text.translate(trans)\r\n",
        "\r\n",
        "      paragraph_list = [p.text for p in soup_obj.find('span', {'itemprop': 'articleBody'}).find_all('p')]\r\n",
        "      paragraph = ''.join(paragraph_list).translate(trans)\r\n",
        "    \r\n",
        "      article = {\r\n",
        "        'category': categories[number],\r\n",
        "        'title': title,\r\n",
        "        'url': url,\r\n",
        "        'sentence': paragraph\r\n",
        "      }\r\n",
        "      print(article) # log\r\n",
        "      articles.append(article)\r\n",
        "    except AttributeError as e:\r\n",
        "      print(\"error\")\r\n",
        "      continue    \r\n",
        "  # break\r\n",
        "\r\n",
        "# print(articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv0cCm5N4pu3"
      },
      "source": [
        "import csv\r\n",
        "\r\n",
        "csv_url = './sample.csv'\r\n",
        "\r\n",
        "with open(csv_url, mode='w', encoding='utf-8') as f:\r\n",
        "  writer = csv.DictWriter(f, ['category', 'title', 'url', 'sentence'])\r\n",
        "  writer.writeheader()\r\n",
        "  writer.writerows(articles)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfaQ4QcWA8K7"
      },
      "source": [
        "with open(csv_url) as f:\r\n",
        "  reader = csv.reader(f)\r\n",
        "  for row in reader:\r\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}