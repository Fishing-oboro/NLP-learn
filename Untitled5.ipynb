{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMYEDHSCCqGPveOQeic1sOy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl5X2s36PuQ5",
        "outputId": "f17ff4f7-681b-40d7-8b1f-41494cab7249"
      },
      "source": [
        "# 必要ライブラリの取得\r\n",
        "!pip install beautifulsoup4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhqR_6uNR3z2"
      },
      "source": [
        "# ニュース記事のURL取得\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import requests\r\n",
        "import time\r\n",
        "\r\n",
        "categories = ['dom', 'world', 'eco', 'ent', 'sports', 'gourmet', 'love', 'trend']\r\n",
        "base_url = \"https://news.livedoor.com/topics/category/\"\r\n",
        "headers = {\r\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36 Edg/89.0.774.48\"\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "def soup(url):\r\n",
        "  time.sleep(3)\r\n",
        "  headers = {\r\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36 Edg/89.0.774.48\"\r\n",
        "  }\r\n",
        "  \r\n",
        "\r\n",
        "  html = requests.get(url, headers = headers)\r\n",
        "  soup = BeautifulSoup(html.content, \"html.parser\")\r\n",
        "  return soup\r\n",
        "\r\n",
        "\r\n",
        "# カテゴリごとのURL取得\r\n",
        "urllist = []\r\n",
        "# page数指定\r\n",
        "page = 1\r\n",
        "\r\n",
        "for i, category  in enumerate(categories):\r\n",
        "  print(categories[i])\r\n",
        "  category_url = base_url + categories[i] + '/'\r\n",
        "\r\n",
        "  # 1page分\r\n",
        "  urls = [topic_soup.find('a').attrs[\"href\"].replace('topics', 'article') for topic_soup in soup(category_url).findAll('li', {'class': 'hasImg'})]\r\n",
        "  # 1のときは無し、2から追加\r\n",
        "  n = 2\r\n",
        "  while(page >= n):\r\n",
        "    print('start')\r\n",
        "    next = '?p=' + str(n)\r\n",
        "    next_url = category_url + next\r\n",
        "    a_urls = [topic_soup.find('a').attrs[\"href\"].replace('topics', 'article') for topic_soup in soup(next_url).findAll('li', {'class': 'hasImg'})]\r\n",
        "    urls.extend(a_urls)\r\n",
        "    n += 1\r\n",
        "\r\n",
        "  urllist.append(urls)\r\n",
        "  print(urls)\r\n",
        "\r\n",
        "print(urllist)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_Ir2Z5D-USs"
      },
      "source": [
        "\r\n",
        "# カテゴリ、タイトル、本文、取得　→　辞書型\r\n",
        "d = {'\\u3000': None, '\\n': None, ' ': None}\r\n",
        "trans = str.maketrans(d)\r\n",
        "\r\n",
        "articles = []\r\n",
        "for number, urls in enumerate(urllist):\r\n",
        "  print(categories[number])\r\n",
        "  for url in urls:\r\n",
        "    try:\r\n",
        "      soup_obj = soup(url)\r\n",
        "      if soup_obj.original_encoding == 'windows-1252':\r\n",
        "        print('error')\r\n",
        "        continue\r\n",
        "\r\n",
        "      title = soup_obj.find('h1', {'class': 'articleTtl'}).text.translate(trans)\r\n",
        "\r\n",
        "      paragraph_list = [p.text for p in soup_obj.find('span', {'itemprop': 'articleBody'}).find_all('p')]\r\n",
        "      paragraph = ''.join(paragraph_list).translate(trans)\r\n",
        "    \r\n",
        "      article = {\r\n",
        "        'category': categories[number],\r\n",
        "        'title': title,\r\n",
        "        'url': url,\r\n",
        "        'sentence': paragraph\r\n",
        "      }\r\n",
        "      print(article) # log\r\n",
        "      articles.append(article)\r\n",
        "    except AttributeError as e:\r\n",
        "      print(\"error\")\r\n",
        "      continue    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv0cCm5N4pu3"
      },
      "source": [
        "# csv作成\r\n",
        "import csv\r\n",
        "\r\n",
        "csv_url = './sample.csv'\r\n",
        "\r\n",
        "with open(csv_url, mode='w', encoding='utf-8') as f:\r\n",
        "  writer = csv.DictWriter(f, ['category', 'title', 'url', 'sentence'])\r\n",
        "  writer.writeheader()\r\n",
        "  writer.writerows(articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfaQ4QcWA8K7"
      },
      "source": [
        "# 確認\r\n",
        "with open(csv_url) as f:\r\n",
        "  reader = csv.reader(f)\r\n",
        "  for row in reader:\r\n",
        "    print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHOA6Tyo0lqq"
      },
      "source": [
        "↑ココまでがニュース記事のcsvファイルの作成  \r\n",
        "↓ここからが分類モデルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWXi2TTq0KP7"
      },
      "source": [
        "import csv\r\n",
        "csv.field_size_limit(1000000000)\r\n",
        "\r\n",
        "csv_url = './sample1page.csv'\r\n",
        "\r\n",
        "categories = []\r\n",
        "titles = []\r\n",
        "articles = []\r\n",
        "with open(csv_url) as f:\r\n",
        "  reader = csv.DictReader(f)\r\n",
        "  # 辞書から本文配列を取得 →　articles = type(list)\r\n",
        "  for row in reader:\r\n",
        "    categories.append(row['category'])\r\n",
        "    titles.append(row['title'])\r\n",
        "    articles.append(row['sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVhW7_703kgp"
      },
      "source": [
        "# mecabに必要なライブラリ取得\r\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\r\n",
        "!pip install mecab-python3\r\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoqH8fGA2cKa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "102d4bc6-9bf8-42b8-d37b-b488cfaf8af9"
      },
      "source": [
        "import MeCab\r\n",
        "from gensim.models import word2vec\r\n",
        "\r\n",
        "corpus = []\r\n",
        "# \"\"\"\r\n",
        "# 単純に分かち書き\r\n",
        "mecab = MeCab.Tagger('-Owakati')\r\n",
        "\r\n",
        "for article in articles:\r\n",
        "  wakati = mecab.parse(article).split(' ')\r\n",
        "  corpus.append(wakati)\r\n",
        "\r\n",
        "\"\"\"\r\n",
        "# 指定品詞(名詞)のみを取り出して分かち書き　←　処理長い\r\n",
        "mecab = MeCab.Tagger('-Ochasen')\r\n",
        "mecab.parse('')\r\n",
        "\r\n",
        "words = []\r\n",
        "for article in articles:\r\n",
        "  node = mecab.parseToNode(article)\r\n",
        "  while node:\r\n",
        "    word_type = node.feature.split(\",\")[0]\r\n",
        "    if word_type in [\"名詞\"]:\r\n",
        "      words.append(node.surface)\r\n",
        "      node = node.next\r\n",
        "  corpus.append(words)\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# 指定品詞(名詞)のみを取り出して分かち書き\\u3000←\\u3000処理長い\\nmecab = MeCab.Tagger(\\'-Ochasen\\')\\nmecab.parse(\\'\\')\\n\\nwords = []\\nfor article in articles:\\n  node = mecab.parseToNode(article)\\n  while node:\\n    word_type = node.feature.split(\",\")[0]\\n    if word_type in [\"名詞\"]:\\n      words.append(node.surface)\\n      node = node.next\\n  corpus.append(words)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHL_MjBm5bm9"
      },
      "source": [
        "# 単語分散表現の生成\r\n",
        "model = word2vec.Word2Vec(corpus)\r\n",
        "model.save(\"./livedoor.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTbZIROHKMi1"
      },
      "source": [
        "モデルの利用\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJz-oVK3JXUr"
      },
      "source": [
        "# 単語分散表現の取得\r\n",
        "from gensim.models import word2vec\r\n",
        "\r\n",
        "model = word2vec.Word2Vec.load(\"./livedoor.model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIQ7u1Kl5-rl"
      },
      "source": [
        "# test\r\n",
        "for i in model.most_similar('日本'):\r\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IIjLaEkn58x"
      },
      "source": [
        "# 各文章ベクトル取得(wv平均)\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "dvecs = []\r\n",
        "\r\n",
        "for i, sentence in enumerate(corpus):\r\n",
        "  wvecs = np.array([model.wv[word] for word in sentence if word in model.wv.vocab.keys()])\r\n",
        "  dvecs.append(np.mean(wvecs, axis = 0)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "EBZInFQ1pwf3",
        "outputId": "61ff46fd-7ec2-4c64-db86-57c293fce48a"
      },
      "source": [
        "# (TF-IDF)作成中\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "vectorizer = TfidfVectorizer()\r\n",
        "np.array(corpus)\r\n",
        "tfvec = vectorizer.fit_transform(np.ravel(np.array(corpus)))\r\n",
        "tfvec"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-478bdbad0ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtfvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtfvec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \"\"\"\n\u001b[1;32m   1858\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1220\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luEgiNRYPnCR"
      },
      "source": [
        "多層パーセプトロンを利用した判定\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bevDWY76q6g0"
      },
      "source": [
        "# 分類器の生成・保存\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "import numpy as np\r\n",
        "import pickle\r\n",
        "\r\n",
        "clf = MLPClassifier(verbose = True)\r\n",
        "clf.fit(dvecs, categories)\r\n",
        "\r\n",
        "with open('model.pickle', mode='wb') as f:\r\n",
        "  pickle.dump(clf,f,protocol=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjSLBX25OtQJ"
      },
      "source": [
        "# 分類器の読み込み\r\n",
        "import pickle\r\n",
        "\r\n",
        "with open('model.pickle', mode='rb') as f:\r\n",
        "  clf = pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlR-6de2RtCK",
        "outputId": "a4d577d4-75d1-4cf7-d347-a6656ef1f2fd"
      },
      "source": [
        "# test\r\n",
        "\r\n",
        "predict = [clf.predict([dvec])[0] for dvec in dvecs]\r\n",
        "print(predict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dom', 'dom', 'dom', 'sports', 'world', 'eco', 'love', 'sports', 'sports', 'world', 'sports', 'world', 'dom', 'love', 'dom', 'trend', 'world', 'world', 'world', 'sports', 'ent', 'ent', 'world', 'dom', 'eco', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'world', 'dom', 'world', 'trend', 'eco', 'gourmet', 'trend', 'love', 'gourmet', 'love', 'eco', 'world', 'love', 'love', 'sports', 'love', 'sports', 'gourmet', 'eco', 'sports', 'ent', 'world', 'world', 'ent', 'ent', 'ent', 'ent', 'ent', 'ent', 'love', 'sports', 'ent', 'dom', 'world', 'ent', 'ent', 'sports', 'dom', 'sports', 'ent', 'sports', 'sports', 'sports', 'dom', 'sports', 'sports', 'eco', 'world', 'sports', 'sports', 'sports', 'gourmet', 'ent', 'world', 'sports', 'love', 'gourmet', 'gourmet', 'gourmet', 'love', 'gourmet', 'gourmet', 'gourmet', 'world', 'love', 'trend', 'gourmet', 'trend', 'gourmet', 'sports', 'eco', 'gourmet', 'gourmet', 'trend', 'ent', 'love', 'love', 'sports', 'eco', 'love', 'love', 'eco', 'love', 'ent', 'love', 'love', 'love', 'love', 'love', 'love', 'love', 'trend', 'trend', 'ent', 'ent', 'trend', 'sports', 'eco', 'ent', 'love', 'love', 'trend', 'dom', 'eco', 'gourmet', 'love', 'trend', 'trend', 'trend', 'trend']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUgRSg5akpe9"
      },
      "source": [
        "# test\r\n",
        "text = 'FC東京のMF東慶悟（30）が15日、本拠地・湘南戦（17日・味スタ）に向けてオンライン取材に応じ、揺るぎない信念を語った。今季はリーグ4試合を終え、1勝2分け1敗の勝ち点5で10位。東は、この現状を「いいスタートとは言えない」と首を振り、こう続けた。「最初から完璧なチームはない。何が良くて悪いかを分析し、チームとしてクリアしたい」全試合得点を続ける一方で、毎試合失点も重ねている。目標に掲げるリーグ60得点ペースではあるが、守備のほころびも見逃せない。「ゴールを目指せば、失点のリスクも高まる。守備に重きを置くと、攻撃ができなくなる。そのバランスを模索している段階。そこがかみ合ってくると信じて続けたい」2018年に長谷川監督が就任して以来、一貫してきた直線的な攻撃と、強固な守備の戦術がある。その土台が強固だからこそ、主将は言い切る。「勝ち試合を落とすと、本当にいいものが悪く感じることがある。考え過ぎずに、今までやってきたことを辛抱強くやっていく。ぶれないことが重要だと思う」まずは、湘南戦。前節は今季初めて先制できたからこそ、勝ちきることが喫緊の課題だ。「一つ一つ（課題を）クリアしてチームが良くなる姿を見せたい」。青赤の主将は小さな成功体験を積み重ね、チームと共に日進月歩の成長を目指す。'\r\n",
        "\r\n",
        "import MeCab\r\n",
        "from gensim.models import word2vec\r\n",
        "\r\n",
        "# 単純に分かち書き\r\n",
        "mecab = MeCab.Tagger('-Owakati')\r\n",
        "\r\n",
        "text_wakati = mecab.parse(text).split(' ')\r\n",
        "\r\n",
        "# 文章ベクトル取得(wv平均)\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "wvecs = np.array([model.wv[word] for word in text_wakati if word in model.wv.vocab.keys()])\r\n",
        "vec = np.mean(wvecs, axis = 0)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DbktXjCqmEFU",
        "outputId": "be52da82-4c90-45ca-dce3-532a7dbb559b"
      },
      "source": [
        "clf.predict([vec])[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sports'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    }
  ]
}