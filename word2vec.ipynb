{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzWDFpiD7MHlSzskB3t/H2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fishing-oboro/NLP-learn/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKB114IS7Vv4"
      },
      "source": [
        "# word2vec (推論ベース)\r\n",
        "***\r\n",
        "word2vecによって単語の分散表現(ベクトル)を得ることができる。単語をベクトルして扱うことでcos類似度や加算減算への利用が可能になる。これには、**CBOW**と**skip-gram**という二つの手法がある。二つの手法はどちらも**分布仮説**に基づいており、一つの単語とその周辺の単語の関係性から特徴量を作成する。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeNy2SDU3EtJ"
      },
      "source": [
        "## CBOW(Continious Bag of Words)\r\n",
        "***\r\n",
        "この手法では**周辺単語(コンテキスト)から中心単語(ターゲット)**を推測することを目的としたニューラルネットワークを活用する。ニューラルネットワークの推論精度が高まるほど正確な単語分散表現を得ることができる。このCBOWの仕組みについて以下の文章を例に考える。  \r\n",
        "\r\n",
        "　　　　　　　**「私　は　リンゴ　を　食べる　。」**\r\n",
        "\r\n",
        "まず、使用される全単語をもった辞書を作成し、one-hotベクトルとして扱う。例：(私→0　[1 0 0 0 0 0], 　リンゴ→2　[0 0 1 0 0 0])  \r\n",
        "次に一つの単語とその周辺単語の組を作成し、それぞれを下記のような正解ラベル、コンテキストとして扱う。\r\n",
        "\r\n",
        "| コンテキスト(入力) | 正解ラベル(出力) | \r\n",
        "| ---- | :----: |\r\n",
        "| 私[0], 　　　リンゴ[2] | は[1] |\r\n",
        "| は[1], 　　　を[3] | リンゴ[2] |\r\n",
        "| リンゴ[2],　食べる[4] | を[3] |\r\n",
        "| を[3],　　　。[5] | 食べる[4] |\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "この表のデータを利用して下記のようなニューラルネットワークを作成し、WinもしくはWoutを単語分散表現として得ることができる。このときニューラルネットワークの出力はスコアを示しているが、softMax関数を利用することで各行の重みの和を1とすることができ、正答単語の確率として扱うことができる。この確率と正解ラベルの誤差を小さくすることで推測の精度を高めることができる。\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/image/CBOW.jpg\" width=80%>\r\n",
        "</div>\r\n",
        "\r\n",
        "### $W_{in}$がなぜ単語分散表現になるのか\r\n",
        "---\r\n",
        "私$\\begin{matrix}(1&0&0&0&0&0)\\end{matrix}$とWinの積は以下のような式になる。  \r\n",
        "\r\n",
        "$$\r\n",
        "\\begin{matrix}(1&0&0&0&0&0)\\end{matrix}\r\n",
        "×\\left(\\begin{matrix}\r\n",
        "\\omega_{11}&\\omega_{12}&...&\\omega_{1n}\\\\\r\n",
        "\\omega_{21}&\\omega_{22}&...&\\omega_{2n}\\\\\r\n",
        "\\omega_{31}&\\omega_{32}&...&\\omega_{3n}\\\\\r\n",
        "\\omega_{41}&\\omega_{42}&...&\\omega_{4n}\\\\\r\n",
        "\\omega_{51}&\\omega_{52}&...&\\omega_{5n}\\\\\r\n",
        "\\omega_{61}&\\omega_{62}&...&\\omega_{6n}\\\\\r\n",
        "\\end{matrix}\\right)\r\n",
        "=\\begin{matrix}(\\omega_{11}&\\omega_{12}&...&\\omega_{1n})\\end{matrix}\r\n",
        "$$\r\n",
        "  \r\n",
        "\r\n",
        "このことからone-hot表現と重み行列の積は単語を示すidの行を取り出すことを意味していることがわかる。したがって、重み行列の各行は辞書に登録されている単語を示す。\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bisP-72MAMSa"
      },
      "source": [
        "\r\n",
        "## skip-gram\r\n",
        "***\r\n",
        "この手法ではCBOWとは逆に**中心単語から周辺単語**を推測するニューラルネットワークによって分散表現を得る。このskip-gramについてCBOWで用いた物と同様の例文を用いて考える。\r\n",
        "\r\n",
        " \r\n",
        "　　　　　　　**「私　は　リンゴ　を　食べる　。」**\r\n",
        "\r\n",
        "CBOWと同様に使用される全単語をもった辞書を作成し、one-hotベクトルとして扱う。例：(私→0　[1 0 0 0 0 0], 　リンゴ→2　[0 0 1 0 0 0])  \r\n",
        "次に一つの単語とその周辺単語の組を作成し、それぞれを下記のような正解ラベル、コンテキストとして扱う。\r\n",
        "\r\n",
        "| コンテキスト(入力) | 正解ラベル(出力) | \r\n",
        "| :----: | ---- |\r\n",
        "| は[1]  |私[0], 　　　リンゴ[2] |\r\n",
        "| リンゴ[2]  |は[1], 　　　を[3] |\r\n",
        "| を[3]  |リンゴ[2],　食べる[4] |\r\n",
        "| 食べる[4]  |を[3],　　　。[5] |\r\n",
        "\r\n",
        "この表のデータを利用して下記のようなニューラルネットワークを作成し、WinもしくはWoutを単語分散表現として得ることができる。このときニューラルネットワークの出力はスコアを示しているが、softMax関数を利用することで各行の重みの和を1とすることができ、正答単語の確率として扱うことができる。この確率と正解ラベルの誤差を小さくすることで推測の精度を高めることができる。\r\n",
        "<div align=\"center\">\r\n",
        "<img src=\"https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/image/skipgram.p.jpg\" width=80%>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gXVdfACiT5z"
      },
      "source": [
        "## 実装例\r\n",
        "---\r\n",
        "例文には青空文庫の[こころ(夏目漱石)](https://www.aozora.gr.jp/cards/000148/card773.html#download)を利用する。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtKuyHEtSQ0r"
      },
      "source": [
        "mecabの出力フォーマットのオプションについては[MeCabのコマンドライン引数一覧とその実行例](http://www.mwsoft.jp/programming/munou/mecab_command.html)に詳しく記載されている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5dyjauxiTjz"
      },
      "source": [
        "# mecabに必要なライブラリ取得\r\n",
        "!apt-get install mecab libmecab-dev mecab-ipadic-utf8\r\n",
        "!pip install mecab-python3\r\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LaRAAXoQ9c6"
      },
      "source": [
        "# テキストの取得\r\n",
        "!wget https://raw.githubusercontent.com/Fishing-oboro/NLP-learn/main/index/statics/kokoro.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68LESIchvQIF"
      },
      "source": [
        "# テキストの加工\r\n",
        "with open(\"./kokoro.txt\", mode=\"r\") as f:\r\n",
        "    text = f.readlines()\r\n",
        "    for line in text:\r\n",
        "        if \"。\" in line:\r\n",
        "          with open(\"./kokoro_new.txt\", mode=\"a\") as of:\r\n",
        "            of.write(line)\r\n",
        "\r\n",
        "# 基本形で分かち書きにする\r\n",
        "!mecab -F\"%f[6] \" -U\"%m \" -E\"\\n\" -o kokoro_wakati.txt kokoro_new.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0b4qUAeq6_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OQdMbYU658h"
      },
      "source": [
        "# モデルの作成\r\n",
        "from gensim.models import word2vec\r\n",
        "docs = word2vec.LineSentence(\"kokoro_wakati.txt\") \r\n",
        "model = word2vec.Word2Vec(docs,\r\n",
        "                        size=100,\r\n",
        "                        min_count=1,\r\n",
        "                        window=5,\r\n",
        "                        iter=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A5D-0PV2DxI"
      },
      "source": [
        "# 類義語の確認\r\n",
        "for i in model.most_similar('母'):\r\n",
        "    print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAuD5dJoH3DM"
      },
      "source": [
        "### word2vecを使った文章のベクトル化手法の例\r\n",
        "---\r\n",
        "1. 文章に登場する単語のベクトルの平均を利用する。  \r\n",
        "2. TF-IDFと組み合わせる。\r\n",
        "\r\n",
        "この二つの方法で以下の3つの文章を使って考える。\r\n",
        "- 私はリンゴとリンゴを食べる\r\n",
        "- 私はリンゴとミカンを食べる\r\n",
        "- 私は虫を食べない\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "Oj8xq1j5JTPH",
        "outputId": "c4d3a69c-bd27-46d7-8955-f852a2a0d81c"
      },
      "source": [
        "import MeCab\r\n",
        "from gensim.models import word2vec\r\n",
        "\r\n",
        "texts = ['私はリンゴとリンゴを食べる', '私はリンゴとミカンを食べる', '私は虫を食べない']\r\n",
        "texts_wakati = [MeCab.Tagger('-Owakati').parse(text).split(' ') for text in texts]\r\n",
        "print(texts_wakati)\r\n",
        "\r\n",
        "# \\nの除去\r\n",
        "data = []\r\n",
        "for text in texts_wakati:\r\n",
        "  words = [word for word in text if word != '\\n']\r\n",
        "  data.append(words)\r\n",
        "  \r\n",
        "model = word2vec.Word2Vec(data)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['私', 'は', 'リンゴ', 'と', 'リンゴ', 'を', '食べる', '\\n'], ['私', 'は', 'リンゴ', 'と', 'ミカン', 'を', '食べる', '\\n'], ['私', 'は', '虫', 'を', '食べ', 'ない', '\\n']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-161a7f25ec33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    }
  ]
}